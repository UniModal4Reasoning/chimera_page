<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="Chimera">
  <meta name="keywords" content="Large Multi-modal Models, multi-modal reasoning, visual structural extraction">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>LLaVA</title>

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.9.1/css/bulma.min.css">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/pure_logo.png">
  <link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">


  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/js/all.min.js"></script>
  <script type="module" src="https://gradio.s3-us-west-2.amazonaws.com/4.16.0/gradio.js"></script>
</head>

<style>
  .expandable-card .card-text-container {
    max-height: 200px;
    overflow-y: hidden;
    position: relative;
  }

  .expandable-card.expanded .card-text-container {
    max-height: none;
  }

  .expand-btn {
    position: relative;
    display: none;
    background-color: rgba(255, 255, 255, 0.8);
    /* margin-top: -20px; */
    /* justify-content: center; */
    color: #510c75;
    border-color: transparent;
  }

  .expand-btn:hover {
    background-color: rgba(200, 200, 200, 0.8);
    text-decoration: none;
    border-color: transparent;
    color: #510c75;
  }

  .expand-btn:focus {
    outline: none;
    text-decoration: none;
  }

  .expandable-card:not(.expanded) .card-text-container:after {
    content: "";
    position: absolute;
    bottom: 0;
    left: 0;
    width: 100%;
    height: 90px;
    background: linear-gradient(rgba(255, 255, 255, 0.2), rgba(255, 255, 255, 1));
  }

  .expandable-card:not(.expanded) .expand-btn {
    margin-top: -40px;
  }

  .card-body {
    padding-bottom: 5px;
  }

  .vertical-flex-layout {
    justify-content: center;
    align-items: center;
    height: 100%;
    display: flex;
    flex-direction: column;
    gap: 5px;
  }

  .figure-img {
    max-width: 100%;
    height: auto;
  }

  .adjustable-font-size {
    font-size: calc(0.5rem + 2vw);
  }

  .chat-history {
    flex-grow: 1;
    overflow-y: auto;
    /* overflow-x: hidden; */
    padding: 5px;
    border-bottom: 1px solid #ccc;
    margin-bottom: 10px;
  }

  #gradio pre {
    background-color: transparent;
  }
</style>

<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Chimera: <span class="is-size-2">Improving Generalist Model with <br>Domain-Specific Experts</span></h1>
            <h5 class="subtitle is-5 publication-awards">Arxiv 2024</h5>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://pengts.github.io/" style="color:#f68946;font-weight:normal;">Tianshuo Peng<sup>1,2,*</sup></a>,
              </span>
              <span class="author-block">
                <a href="https://msheng-lee.github.io/" style="color:#008AD7;font-weight:normal;">Mingsheng Li<sup>1,3,*</sup></a>,
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=fb_WgAEAAAAJ&hl=en&oi=sra" style="color:#F2A900;font-weight:normal;">Hongbin Zhou<sup>1</sup></a>,
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=E520fqQAAAAJ&hl=zh-CN" style="color:#F2A900;font-weight:normal;">Renqiu Xia<sup>1,4</sup></a>,
              </span>
              <span class="author-block">
                <a href="https://zrrskywalker.github.io/" style="color:#F2A900;font-weight:normal;">Renrui Zhang<sup>2</sup></a>,
              </span>
              <span class="author-block">
                <a href="http://leibai.site/" style="color:#F2A900;font-weight:normal;">Lei Bai<sup>1</sup></a>,
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=BaqGkQQAAAAJ&hl=zh-CN" style="color:#F2A900;font-weight:normal;">Song Mao<sup>1</sup></a>,
              </span>
              <span class="author-block">
                <a href="https://wangbindl.github.io/" style="color:#F2A900;font-weight:normal;">Bin Wang<sup>1</sup></a>,
              </span>
              <span class="author-block">
                <a href="https://conghui.github.io/" style="color:#F2A900;font-weight:normal;">Conghui He<sup>1</sup></a>,
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com.hk/citations?user=cC8lXi8AAAAJ&hl=en" style="color:#F2A900;font-weight:normal;">Aojun Zhou<sup>2</sup></a>,
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=K0PpvLkAAAAJ&hl=en" style="color:#F2A900;font-weight:normal;">Botian Shi<sup>1</sup></a>,
              </span>
              <span class="author-block">
                <a href="https://eetchen.github.io/" style="color:#F2A900;font-weight:normal;">Tao Chen<sup>3</sup></a>,
              </span>
              <span class="author-block">
                <a href="https://bobrown.github.io/boZhang.github.io/" style="color:#F2A900;font-weight:normal;">Bo Zhang<sup>1,✉,&#x2021;</sup></a>,
              </span>
              <span class="author-block">
                <a href="https://www.ie.cuhk.edu.hk/faculty/yue-xiangyu/" style="color:#F2A900;font-weight:normal;">Xiangyu Yue<sup>2,✉</sup></a>,
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>Shanghai Artificial Intelligence Laboratory,</span>
              <span class="author-block"><sup>2</sup>MMLab, The Chinese University of Hong Kong,</span><br>
              <span class="author-block"><sup>3</sup>Fudan University,</span>
              <span class="author-block"><sup>4</sup>Shanghai Jiao Tong University</span><br>
            </div>
 
            <div class="is-size-5 publication-authors">
              <sup>*</sup> Equal contribution,  <sup>✉</sup> Corresponding author,  <sup>&#x2021;</sup> Project Leader
            </div>


<!--             <div class="is-size-5 publication-authors">
              <span class="author-block"><b style="color:#f68946; font-weight:normal">&#x25B6 </b> University of
                Wisconsin-Madison</b></span>
              <span class="author-block"><b style="color:#008AD7; font-weight:normal">&#x25B6 </b> Microsoft Research</span>
              <span class="author-block"><b style="color:#F2A900; font-weight:normal">&#x25B6 </b>Columbia
                University</span>
            </div>
 -->

            
          <!-- <div class="column has-text-centered">
            <h3 class="title is-3 publication-title">Improved Baselines with Visual Instruction Fine-tuning</h3>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://hliu.cc/" style="color:#f68946;font-weight:normal;">Haotian Liu<sup>*</sup></a>,
              </span>
              <span class="author-block">
                <a href="https://chunyuan.li/" style="color:#008AD7;font-weight:normal;">Chunyuan Li<sup>*</sup></a>,
              </span>
              <span class="author-block">
                <a href="https://yuheng-li.github.io" style="color:#008AD7;font-weight:normal;">Yuheng Li</a>,
              </span>
              <span class="author-block">
                <a href="https://pages.cs.wisc.edu/~yongjaelee/" style="color:#f68946;font-weight:normal;">Yong Jae
                  Lee</a>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><b style="color:#f68946; font-weight:normal">&#x25B6 </b> University of
                Wisconsin-Madison</b></span>
              <span class="author-block"><b style="color:#008AD7; font-weight:normal">&#x25B6 </b> Microsoft Research</span>
            </div> -->

            <div class="column has-text-centered">
              <div class="publication-links">
                <span class="link-block">
                  <a href="" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>ArXiv (Chimera-1.0)</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://github.com/UniModal4Reasoning/Chimera" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
<!--                 <span class="link-block">
                  <a href="https://llava.hliu.cc" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="far fa-images"></i>
                    </span>
                    <span>Demo</span>
                  </a>
                </span> -->
<!--                 <span class="link-block">
                  <a href="https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-database"></i>
                    </span>
                    <span>Dataset</span>
                  </a>
                </span> -->
                <span class="link-block">
                  <a href="https://huggingface.co/collections/U4R/chimera-10-6749542e2f0dfa09414232c0" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-share-square"></i>
                    </span>
                    <span>Model</span>
                  </a>
                </span>

                <!-- <span class="link-block">
                <a href="#"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                  </a>
              </span> -->
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <h4 class="subtitle">
          🔥<span style="color: #ff3860">[NEW!]</span> 
          Chimera-Reasoner-8B gets 64.9 on MathVista, achieves new SoTA under 10B scale models! <br>
          🔥<span style="color: #ff3860">[NEW!]</span>
          Chimera-Extractor demonstrates powerful extraction performance on various types of documents! <br>
          🔥<span style="color: #ff3860">[NEW!]</span>
          <a href="https://huggingface.co/collections/U4R/chimera-10-6749542e2f0dfa09414232c0">Weights</a> & <a href="https://github.com/UniModal4Reasoning/Chimera">Inference code</a> have been released！
        </h4>
      </div>
    </div>
  </section>



<section class="section">
  <div class="container" style="margin-bottom: 2vh;">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Emerging Capabilities</h2>
        <div class="content has-text-justified">
        <p>
        In addition to reporting the LLaVA-OneVision’s capabilities across various benchmarks, we also observe the emerging behaviors of the proposed model with task transfer and composition, paving a promising way to generalize to tackle real-world computer vision tasks in the wild. We illustrate several emerging capabilities using examples as below.
      </p>
        <div id="results-carousel" class="carousel results-carousel">

          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="demos/df.png" alt="algebraic reasoning" width="100%"/>
              <p> LLaVA-OneVision transfers its ability to understand diagram and table to multi-image scenarios, interpreting multiple images in a coherent manner.
            </div>
          </div>

          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="demos/gui.png" alt="arithmetic reasoning" width="80%"/>
              <p> LLaVA-OneVision plays the role of agent. It recognizes multiple screenshots on the iPhone and take action to interact with the iPhone, providing operation instructions for automating tasks.
            </div>
          </div>

          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="demos/som.png" alt="arithmetic reasoning" width="100%"/>
              <p> LLaVA-OneVision exhibits excellent set-of-mark prompting capabilities, ie, referring to marks when answering questions. This example demonstrates that describing specific objects based on numerical labels within an image highlights its comprehension skills in handling fine-grained visual content.
            </div>
          </div>


          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="demos/i2v.png" alt="arithmetic reasoning" width="100%"/>
              <p> 
                LLaVA-OneVision learns to generate detailed video creation prompts based on a static image. This capability is generalized to videos from the image-to-image language editing generation.
              </div>
          </div>


          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="demos/video2.png" alt="arithmetic reasoning" width="100%"/>
              <p> 
                LLaVA-OneVision learns to analyze differences between videos with the same starting frame but different endings.</div>
          </div>


          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="demos/video.png" alt="arithmetic reasoning" width="80%"/>
              <p> 
                LLaVA-OneVision learns to analyze differences between videos with similar backgrounds but different foreground objects.</div>
          </div>


          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="demos/selfd.png" alt="arithmetic reasoning" width="50%"/>
              <p> 
                LLaVA-OneVision analyzes and interprets multi-camera video footage in self-driving contexts.</div>
          </div>

          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="demos/subv.png" alt="arithmetic reasoning" width="60%"/>
              <p> 
                LLaVA-OneVision learns to understand and describe composed sub-videos in detail.</div>
          </div>


          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="demos/vp.png" alt="arithmetic reasoning" width="50%"/>
              <p> 
                LLaVA-OneVision learns to provide detailed descriptions of highlighted subjects in video content.</div>
          </div>


          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="demos/messi.png" alt="arithmetic reasoning" width="50%"/>
              <p> 
                LLaVA-OneVision’s capability in referring image and video understanding. It accurately identifies the same individual in two images in the first instance. It identifies the same individual in both the image and the video in the second instance and correctly concludes the absence of the individual in the third instance, indicating its understanding capability to relate visual query in both image and video understanding.</div>
          </div>

      </div>
    </div>
    <!--/ Abstract. -->
</div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
              Recent advancements in Large Multi-modal Models (LMMs) underscore the importance of scaling by increasing image-text paired data, achieving impressive performance on general tasks. 
              Despite their effectiveness in broad applications, generalist models are primarily trained on web-scale datasets dominated by natural images, resulting in the sacrifice of specialized capabilities for domain-specific tasks that require extensive domain prior knowledge. 
              Moreover, directly integrating expert models tailored for specific domains is challenging due to the representational gap and imbalanced optimization between the generalist model and experts. 
              To address these challenges, we introduce <b>Chimera</b>, a scalable and low-cost multi-modal pipeline designed to boost the ability of existing LMMs with domain-specific experts. 
              Specifically, we design a progressive training strategy to integrate features from expert models into the input of a generalist LMM. 
              To address the imbalanced optimization caused by the well-aligned general visual encoder, we introduce a novel Generalist-Specialist Collaboration Masking (GSCM) mechanism. 
              This results in a versatile model that excels across the chart, table, math, and document domains, achieving state-of-the-art performance on multi-modal reasoning and visual content extraction tasks, both of which are challenging tasks for assessing existing LMMs. 
              We will release Chimera's weights, along with the data used for training and evaluation, to facilitate future research on LMMs.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-full">
        <h2 class="title is-3">Method</h2>
      </div>
    </div>
  </div>
  <!--/ Results. -->    
<div class="container is-max-desktop">

  <div class="columns is-centered">
    <div class="column is-full-width">
      <div class="content has-text-justified"> 
        <p>
          We introduce Chimera, a <b>scalable</b>b> pipeline that integrates specialist models into generalist LMMs, facilitating their adaptation to diverse specialized tasks. 
          Chimera comprises a general visual encoder, a general projector paired with a language model pre-trained from an LMM, alongside a router and an expert model suite, which includes specialized expert models and their corresponding expert projectors. 
          Chimera uses a <b>Generalist-Specialist Collaboration Masking</b> (GSCM) mechanism to facilitate the alignment with expert models. 
          <br>
          We consider a progressive two-stage training procedure:
          <ul type="1">
            <li><b>Stage 1: Domain-General Knowledge Alignment</b>. <span style="font-size: 95%;">To initially align domain-specific knowledge with the semantic space of the generalist LMM, we train the model using tasks that directly perceive diverse image content. We only train the general projector and expert projectors during this stage.</span></li>
            <li><b>Stage 2: Visual Instruction Tuning</b>. <span style="font-size: 95%;">To further enhance the performance of Chimera on specialized tasks, we perform visual instruction tuning on different domain-specific tasks with the proposed GSCM. All projectors and LLM are updated during this stage.</span></li>
          </ul>  
          Please check out our 
          <a href="https://huggingface.co/collections/U4R/chimera-10-6749542e2f0dfa09414232c0">[Model Zoo]</a>.
        </p>
      </div>
      <centering>
        <div style="text-align: center;">
          <img id="teaser" width="70%" src="static/overview.png">     
        </div>

      
      </centering>           
    </div>
  </div>


</section>
  

<section class="section">
  <div class="container" style="margin-bottom: 2vh;">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Examples</h2>
        <div class="content has-text-justified">
        <p>
          In addition to quantitatively reporting Chimera's performance across various benchmarks, we also provide several demos below to showcase Chimera's capabilities on challenging domain-specific tasks, such as table format transformation, chart structural extraction, and document context extraction.
        </p>
        <div id="results-carousel" class="carousel results-carousel">

          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="static/demo_1.png" alt="algebraic reasoning" width="100%"/>
              <p> Output of Chimera-Reasoner-8B on Table Format Transformation.
            </div>
          </div>

          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="static/demo_2.png" alt="arithmetic reasoning" width="80%"/>
              <p> Output of Chimera-Reasoner-8B on Table Format Transformation.
            </div>
          </div>

          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="static/demo_3.png" alt="arithmetic reasoning" width="100%"/>
              <p> Output of Chimera-Reasoner-8B on Chart Structural Extraction.
            </div>
          </div>


          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="static/demo_4.png" alt="arithmetic reasoning" width="100%"/>
              <p> 
                Output of Chimera-Reasoner-8B on Chart Structural Extraction.
              </div>
          </div>


          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="static/demo_5.png" alt="arithmetic reasoning" width="100%"/>
              <p> 
                Output of Chimera-Reasoner-8B on Document Context Extraction.</div>
          </div>


          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="static/demo_6.png" alt="arithmetic reasoning" width="80%"/>
              <p> 
                Output of Chimera-Reasoner-8B on Document Context Extraction.</div>
          </div>
      </div>
    </div>
    <!--/ Abstract. -->
</div>
</section>
  
  
<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/UniModal4Reasoning/Chimera" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is adapted from <a
              href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>, licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>


<script src="https://code.jquery.com/jquery-3.5.1.js"></script>
<script src="https://cdn.datatables.net/1.10.24/js/jquery.dataTables.min.js"></script>
<script src="https://cdn.datatables.net/1.10.24/js/dataTables.bootstrap4.min.js"></script>
<script>
    $(document).ready(function () {
        $('#nr3d_table').DataTable({
            columnDefs: [ // disable sorting of the first column
                {orderable: false, targets: 0}
            ],
            "order": [[1, "desc"]], // Default Ordering on the overall column
            searching: false, // Disable search bar
            paging: false, // Stop pagination for now
            info: false // hide "Showing 1 to M of N entries" line
        });

        $('#sr3d_table').DataTable({
            columnDefs: [ // disable sorting of the first column
                {orderable: false, targets: 0}
            ],
            "order": [[1, "desc"]], // Default Ordering on the overall column
            searching: false, // Disable search bar
            paging: false, // Stop pagination for now
            info: false // hide "Showing 1 to M of N entries" line
        });
    })
</script>

</body>

</html>
