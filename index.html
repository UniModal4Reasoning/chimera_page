<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="Chimera">
  <meta name="keywords" content="Large Multi-modal Models, multi-modal reasoning, visual structural extraction">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Chimera</title>

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.9.1/css/bulma.min.css">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/pure_logo.png">
  <link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">


  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/js/all.min.js"></script>
  <script type="module" src="https://gradio.s3-us-west-2.amazonaws.com/4.16.0/gradio.js"></script>


  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="stylesheet" href="./static/css/leaderboard.css">

  <!-- <link href="https://unpkg.com/tabulator-tables@5.5.2/dist/css/tabulator_bulma.min.css" rel="stylesheet">
  <script type="text/javascript" src="https://unpkg.com/tabulator-tables@5.5.2/dist/js/tabulator.min.js"></script> -->
  <script type="text/javascript" src="static/js/sort-table.js" defer></script>

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/explorer-index.js"></script>
  <script src="./static/js/question_card.js"></script>

  <script src="./static/js/leaderboard_testmini.js"></script>  
  <script src="./data/results/output_folders.js" defer></script>
  <script src="./data/results/model_scores.js" defer></script>

  <script src="./visualizer/data/data_public.js" defer></script>

  <style>
      .center-container {
            display: flex;
            justify-content: center;
            align-items: center;
            height: 100%;
            margin-top: -20px;
        }
    .node {
      fill: #f8f1e4;
      stroke: #000;
      stroke-width: 1;
      rx: 10;
      ry: 10;
    }
    .node text {
      font-size: 14px;
      text-anchor: middle;
    }
    .link {
      fill: none;
      stroke: #000;
      stroke-width: 2;
    }
    .badge {
      font-size: 12px;
    }
  </style>


<style>
  /* Basic styling for the bar */
  .navbar {
      background-color: #333;
      overflow: hidden;
  }
  
  .navbar a {
      float: left;
      display: block;
      color: #f2f2f2;
      text-align: center;
      padding: 14px 16px;
      text-decoration: none;
  }
  
  .navbar a:hover {
      background-color: #ddd;
      color: black;
  }
</style>


</head>
<body>


  <!--   <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Chimera: <span class="is-size-2">Improving Generalist Model with <br>Domain-Specific Experts</span></h1>
            <h5 class="subtitle is-5 publication-awards has-text-centered">Arxiv 2024</h5>  -->
  
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Chimera: <span class="is-size-2">Improving Generalist Model with <br>Domain-Specific Experts</span></h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://pengts.github.io/" style="color:#f68946;font-weight:normal;">Tianshuo Peng<sup>1,2,*</sup></a>,
              </span>
              <span class="author-block">
                <a href="https://msheng-lee.github.io/" style="color:#008AD7;font-weight:normal;">Mingsheng Li<sup>1,3,*</sup></a>,
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=fb_WgAEAAAAJ&hl=en&oi=sra" style="color:#F2A900;font-weight:normal;">Hongbin Zhou<sup>1</sup></a>,
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=E520fqQAAAAJ&hl=zh-CN" style="color:#F2A900;font-weight:normal;">Renqiu Xia<sup>1,4</sup></a>,
              </span>
              <span class="author-block">
                <a href="https://zrrskywalker.github.io/" style="color:#F2A900;font-weight:normal;">Renrui Zhang<sup>2</sup></a>,
              </span>
              <span class="author-block">
                <a href="http://leibai.site/" style="color:#F2A900;font-weight:normal;">Lei Bai<sup>1</sup></a>,
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=BaqGkQQAAAAJ&hl=zh-CN" style="color:#F2A900;font-weight:normal;">Song Mao<sup>1</sup></a>,
              </span>
              <span class="author-block">
                <a href="https://wangbindl.github.io/" style="color:#F2A900;font-weight:normal;">Bin Wang<sup>1</sup></a>,
              </span>
              <span class="author-block">
                <a href="https://conghui.github.io/" style="color:#F2A900;font-weight:normal;">Conghui He<sup>1</sup></a>,
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com.hk/citations?user=cC8lXi8AAAAJ&hl=en" style="color:#F2A900;font-weight:normal;">Aojun Zhou<sup>2</sup></a>,
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=K0PpvLkAAAAJ&hl=en" style="color:#F2A900;font-weight:normal;">Botian Shi<sup>1</sup></a>,
              </span>
              <span class="author-block">
                <a href="https://eetchen.github.io/" style="color:#F2A900;font-weight:normal;">Tao Chen<sup>3</sup></a>,
              </span>
              <span class="author-block">
                <a href="https://bobrown.github.io/boZhang.github.io/" style="color:#F2A900;font-weight:normal;">Bo Zhang<sup>1,‚úâ,&#x2021;</sup></a>,
              </span>
              <span class="author-block">
                <a href="https://www.ie.cuhk.edu.hk/faculty/yue-xiangyu/" style="color:#F2A900;font-weight:normal;">Xiangyu Yue<sup>2,‚úâ</sup></a>,
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>Shanghai Artificial Intelligence Laboratory,</span>
              <span class="author-block"><sup>2</sup>MMLab, The Chinese University of Hong Kong,</span><br>
              <span class="author-block"><sup>3</sup>Fudan University,</span>
              <span class="author-block"><sup>4</sup>Shanghai Jiao Tong University</span><br>
            </div>
 
            <div class="is-size-5 publication-authors">
              <sup>*</sup> Equal contribution,  <sup>‚úâ</sup> Corresponding author,  <sup>&#x2021;</sup> Project Leader
            </div>


<!--             <div class="is-size-5 publication-authors">
              <span class="author-block"><b style="color:#f68946; font-weight:normal">&#x25B6 </b> University of
                Wisconsin-Madison</b></span>
              <span class="author-block"><b style="color:#008AD7; font-weight:normal">&#x25B6 </b> Microsoft Research</span>
              <span class="author-block"><b style="color:#F2A900; font-weight:normal">&#x25B6 </b>Columbia
                University</span>
            </div>
 -->

            
          <!-- <div class="column has-text-centered">
            <h3 class="title is-3 publication-title">Improved Baselines with Visual Instruction Fine-tuning</h3>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://hliu.cc/" style="color:#f68946;font-weight:normal;">Haotian Liu<sup>*</sup></a>,
              </span>
              <span class="author-block">
                <a href="https://chunyuan.li/" style="color:#008AD7;font-weight:normal;">Chunyuan Li<sup>*</sup></a>,
              </span>
              <span class="author-block">
                <a href="https://yuheng-li.github.io" style="color:#008AD7;font-weight:normal;">Yuheng Li</a>,
              </span>
              <span class="author-block">
                <a href="https://pages.cs.wisc.edu/~yongjaelee/" style="color:#f68946;font-weight:normal;">Yong Jae
                  Lee</a>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><b style="color:#f68946; font-weight:normal">&#x25B6 </b> University of
                Wisconsin-Madison</b></span>
              <span class="author-block"><b style="color:#008AD7; font-weight:normal">&#x25B6 </b> Microsoft Research</span>
            </div> -->

            <div class="column has-text-centered">
              <div class="publication-links">
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2412.05983" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>ArXiv (Chimera-1.0)</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://github.com/UniModal4Reasoning/Chimera" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
<!--                 <span class="link-block">
                  <a href="https://llava.hliu.cc" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="far fa-images"></i>
                    </span>
                    <span>Demo</span>
                  </a>
                </span> -->
<!--                 <span class="link-block">
                  <a href="https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-database"></i>
                    </span>
                    <span>Dataset</span>
                  </a>
                </span> -->
                <span class="link-block">
                  <a href="https://huggingface.co/collections/U4R/chimera-10-6749542e2f0dfa09414232c0" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-share-square"></i>
                    </span>
                    <span>Model</span>
                  </a>
                </span>

                <!-- <span class="link-block">
                <a href="#"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                  </a>
              </span> -->
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <h4 class="subtitle">
          üî•<span style="color: #ff3860">[NEW!]</span> 
          Chimera-Reasoner-8B gets 64.9 on MathVista, achieves new SoTA under 10B scale models! <br>
          üî•<span style="color: #ff3860">[NEW!]</span>
          Chimera-Extractor demonstrates powerful extraction performance on various types of documents! <br>
          üî•<span style="color: #ff3860">[NEW!]</span>
          <a href="https://huggingface.co/collections/U4R/chimera-10-6749542e2f0dfa09414232c0">Weights</a> & <a href="https://github.com/UniModal4Reasoning/Chimera">Inference code</a> have been releasedÔºÅ
        </h4>
      </div>
    </div>
  </section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
              Recent advancements in Large Multi-modal Models (LMMs) underscore the importance of scaling by increasing image-text paired data, achieving impressive performance on general tasks. 
              Despite their effectiveness in broad applications, generalist models are primarily trained on web-scale datasets dominated by natural images, resulting in the sacrifice of specialized capabilities for domain-specific tasks that require extensive domain prior knowledge. 
              Moreover, directly integrating expert models tailored for specific domains is challenging due to the representational gap and imbalanced optimization between the generalist model and experts. 
              To address these challenges, we introduce <b>Chimera</b>, a scalable and low-cost multi-modal pipeline designed to boost the ability of existing LMMs with domain-specific experts. 
              Specifically, we design a progressive training strategy to integrate features from expert models into the input of a generalist LMM. 
              To address the imbalanced optimization caused by the well-aligned general visual encoder, we introduce a novel Generalist-Specialist Collaboration Masking (GSCM) mechanism. 
              This results in a versatile model that excels across the chart, table, math, and document domains, achieving state-of-the-art performance on multi-modal reasoning and visual content extraction tasks, both of which are challenging tasks for assessing existing LMMs. 
              We will release Chimera's weights, along with the data used for training and evaluation, to facilitate future research on LMMs.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-full">
        <h2 class="title is-3">Method</h2>
      </div>
    </div>
  </div>
  <!--/ Results. -->    
<div class="container is-max-desktop">

  <div class="columns is-centered">
    <div class="column is-full-width">
      <div class="content has-text-justified"> 
        <p>
          We introduce Chimera, a <b>scalable</b> pipeline that integrates specialist models into generalist LMMs, facilitating their adaptation to diverse specialized tasks. 
          Chimera comprises a general visual encoder, a general projector paired with a language model pre-trained from an LMM, alongside a router and an expert model suite, which includes specialized expert models and their corresponding expert projectors. 
          Chimera uses a <b>Generalist-Specialist Collaboration Masking</b> (GSCM) mechanism to facilitate the alignment with expert models. 
          <br>
          We consider a progressive two-stage training procedure:
          <ul type="1">
            <li><b>Stage 1: Domain-General Knowledge Alignment</b>. <span style="font-size: 95%;">To initially align domain-specific knowledge with the semantic space of the generalist LMM, we train the model using tasks that directly perceive diverse image content. We only train the general projector and expert projectors during this stage.</span></li>
            <li><b>Stage 2: Visual Instruction Tuning</b>. <span style="font-size: 95%;">To further enhance the performance of Chimera on specialized tasks, we perform visual instruction tuning on different domain-specific tasks with the proposed GSCM. All projectors and LLM are updated during this stage.</span></li>
          </ul>  
          Please check out our 
          <a href="https://huggingface.co/collections/U4R/chimera-10-6749542e2f0dfa09414232c0">[Model Zoo]</a>.
        </p>
      </div>
      <centering>
        <div style="text-align: center;">
          <img id="teaser" width="70%" src="static/overview.png">     
        </div>

      
      </centering>           
    </div>
  </div>
</section>




<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-full">
        <h2 class="title is-3">Performance</h2>
      </div>
    </div>
  </div>
  <!--/ Results. -->    
<div class="container is-max-desktop">

  <div class="columns is-centered">
    <div class="column is-full-width">
      <div class="content has-text-justified"> 
        <p>
          We conduct quantitative experiments to evaluate Chimera's capabilities in multi-modal reasoning and visual content extraction. 
          Chimera achieves a new SOTA for LMMs of comparable scale on two multi-modal reasoning benchmarks. 
          It also surpasses or matches the performance of representative expert models in visual content extraction tasks across chart, table, and document domains.
        </p>
      </div>
      <centering>
        <div style="text-align: center;">
          <img id="teaser" width="70%" src="static/reasoning_mathvista.png">     
        </div>
        <br>
        <div style="text-align: center;">
          <img id="teaser" width="70%" src="static/reasoning_mathverse.png">     
        </div>
        <br>
        <div style="text-align: center;">
          <img id="teaser" width="70%" src="static/extraction_all.png">     
        </div>
      
      </centering>           
    </div>
  </div>
</section>


  

<section class="section">
  <div class="container" style="margin-bottom: 2vh;">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Examples</h2>
        <div class="content has-text-justified">
        <p>
          In addition to quantitatively reporting Chimera's performance across various benchmarks, we also provide several demos below to showcase Chimera's capabilities on challenging domain-specific tasks, such as table format transformation, chart structural extraction, and document context extraction.
        </p>
        <div id="results-carousel" class="carousel results-carousel">

          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="static/demo_1.png" alt="algebraic reasoning" width="100%"/>
              <p> Output of Chimera-Reasoner-8B on Table Format Transformation.
            </div>
          </div>

          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="static/demo_2.png" alt="arithmetic reasoning" width="80%"/>
              <p> Output of Chimera-Reasoner-8B on Table Format Transformation.
            </div>
          </div>

          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="static/demo_3.png" alt="arithmetic reasoning" width="100%"/>
              <p> Output of Chimera-Reasoner-8B on Chart Structural Extraction.
            </div>
          </div>


          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="static/demo_4.png" alt="arithmetic reasoning" width="100%"/>
              <p> 
                Output of Chimera-Reasoner-8B on Chart Structural Extraction.
              </div>
          </div>


          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="static/demo_5.png" alt="arithmetic reasoning" width="100%"/>
              <p> 
                Output of Chimera-Extractor-1B on Document Context Extraction.</div>
          </div>


          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="static/demo_6.png" alt="arithmetic reasoning" width="80%"/>
              <p> 
                Output of Chimera-Extractor-1B on Document Context Extraction.</div>
          </div>
      </div>
    </div>
    <!--/ Abstract. -->
</div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>
      @misc{peng2024chimeraimprovinggeneralistmodel,
        title={Chimera: Improving Generalist Model with Domain-Specific Experts}, 
        author={Tianshuo Peng and Mingsheng Li and Hongbin Zhou and Renqiu Xia and Renrui Zhang and Lei Bai and Song Mao and Bin Wang and Conghui He and Aojun Zhou and Botian Shi and Tao Chen and Bo Zhang and Xiangyu Yue},
        year={2024},
        eprint={2412.05983},
        archivePrefix={arXiv},
        primaryClass={cs.CV},
        url={https://arxiv.org/abs/2412.05983}, 
      }
    </code></pre>
      </div>
</section>
  
<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="https://arxiv.org/abs/2412.05983">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/UniModal4Reasoning/Chimera" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is adapted from <a
              href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>, licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>


<script src="https://code.jquery.com/jquery-3.5.1.js"></script>
<script src="https://cdn.datatables.net/1.10.24/js/jquery.dataTables.min.js"></script>
<script src="https://cdn.datatables.net/1.10.24/js/dataTables.bootstrap4.min.js"></script>
<script>
    $(document).ready(function () {
        $('#nr3d_table').DataTable({
            columnDefs: [ // disable sorting of the first column
                {orderable: false, targets: 0}
            ],
            "order": [[1, "desc"]], // Default Ordering on the overall column
            searching: false, // Disable search bar
            paging: false, // Stop pagination for now
            info: false // hide "Showing 1 to M of N entries" line
        });

        $('#sr3d_table').DataTable({
            columnDefs: [ // disable sorting of the first column
                {orderable: false, targets: 0}
            ],
            "order": [[1, "desc"]], // Default Ordering on the overall column
            searching: false, // Disable search bar
            paging: false, // Stop pagination for now
            info: false // hide "Showing 1 to M of N entries" line
        });
    })
</script>

</body>

</html>
